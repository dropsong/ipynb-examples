{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鸢尾花数据集\n",
    "\n",
    "快速了解鸢尾花数据集：\n",
    "https://www.bilibili.com/video/BV1Ut411n78Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_20newsgroups, fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "<class 'numpy.ndarray'>\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n",
      "目标值\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "--------------------------------------------------\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "--------------------------------------------------\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#鸢尾花数据集，查看特征，目标，样本量\n",
    "\n",
    "li = load_iris()\n",
    "\n",
    "print(\"获取特征值\")\n",
    "print(type(li.data))\n",
    "print(li.data)\n",
    "print(li.data.shape)\n",
    "print(\"目标值\")\n",
    "print(li.target)\n",
    "print('-' * 50)\n",
    "print(li.DESCR)\n",
    "print('-' * 50)\n",
    "print(li.feature_names)  # 重点\n",
    "print('-' * 50)\n",
    "print(li.target_names)\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征值和目标值： [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]] [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "训练集特征值shape (112, 4)\n",
      "测试集特征值和目标值： [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]] [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "测试集特征值shape (38, 4)\n"
     ]
    }
   ],
   "source": [
    "# 默认是乱序的,random_state为了确保两次的随机策略一致，往往会带上\n",
    "# 训练集的特征向量，测试集的特征向量，训练集的目标，测试集的目标\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "# 测试集占比 25%\n",
    "\n",
    "print(\"训练集特征值和目标值：\", x_train, y_train)\n",
    "print(\"训练集特征值shape\", x_train.shape)\n",
    "\n",
    "print(\"测试集特征值和目标值：\", x_test, y_test)\n",
    "print(\"测试集特征值shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 类新闻数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      ".. dropdown:: Usage\n",
      "\n",
      "  The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "  fetching / caching functions that downloads the data archive from\n",
      "  the original `20 newsgroups website <http://people.csail.mit.edu/jrennie/20Newsgroups/>`__,\n",
      "  extracts the archive contents\n",
      "  in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      "  :func:`sklearn.datasets.load_files` on either the training or\n",
      "  testing set folder, or both of them::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_20newsgroups\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "    >>> from pprint import pprint\n",
      "    >>> pprint(list(newsgroups_train.target_names))\n",
      "    ['alt.atheism',\n",
      "     'comp.graphics',\n",
      "     'comp.os.ms-windows.misc',\n",
      "     'comp.sys.ibm.pc.hardware',\n",
      "     'comp.sys.mac.hardware',\n",
      "     'comp.windows.x',\n",
      "     'misc.forsale',\n",
      "     'rec.autos',\n",
      "     'rec.motorcycles',\n",
      "     'rec.sport.baseball',\n",
      "     'rec.sport.hockey',\n",
      "     'sci.crypt',\n",
      "     'sci.electronics',\n",
      "     'sci.med',\n",
      "     'sci.space',\n",
      "     'soc.religion.christian',\n",
      "     'talk.politics.guns',\n",
      "     'talk.politics.mideast',\n",
      "     'talk.politics.misc',\n",
      "     'talk.religion.misc']\n",
      "\n",
      "  The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "  attribute is the integer index of the category::\n",
      "\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "  It is possible to load only a sub-selection of the categories by passing the\n",
      "  list of the categories to load to the\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "    >>> cats = ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "    >>> list(newsgroups_train.target_names)\n",
      "    ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      ".. dropdown:: Converting text to vectors\n",
      "\n",
      "  In order to feed predictive or clustering models with the text data,\n",
      "  one first need to turn the text into vectors of numerical values suitable\n",
      "  for statistical analysis. This can be achieved with the utilities of the\n",
      "  ``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "  example that extract `TF-IDF <https://en.wikipedia.org/wiki/Tf-idf>`__ vectors\n",
      "  of unigram tokens from a subset of 20news::\n",
      "\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "    ...               'comp.graphics', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> vectors.shape\n",
      "    (2034, 34118)\n",
      "\n",
      "  The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "  components by sample in a more than 30000-dimensional space\n",
      "  (less than .5% non-zero features)::\n",
      "\n",
      "    >>> vectors.nnz / float(vectors.shape[0])\n",
      "    159.01327...\n",
      "\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "  returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. dropdown:: Filtering text for more realistic training\n",
      "\n",
      "  It is easy for a classifier to overfit on particular things that appear in the\n",
      "  20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "  high F-scores, but their results would not generalize to other documents that\n",
      "  aren't from this window of time.\n",
      "\n",
      "  For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "  which is fast to train and achieves a decent F-score::\n",
      "\n",
      "    >>> from sklearn.naive_bayes import MultinomialNB\n",
      "    >>> from sklearn import metrics\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.88213...\n",
      "\n",
      "  (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "  the training and test data, instead of segmenting by time, and in that case\n",
      "  multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "  yet of what's going on inside this classifier?)\n",
      "\n",
      "  Let's take a look at what the most informative features are:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def show_top10(classifier, vectorizer, categories):\n",
      "    ...     feature_names = vectorizer.get_feature_names_out()\n",
      "    ...     for i, category in enumerate(categories):\n",
      "    ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "    ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "    ...\n",
      "    >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "    alt.atheism: edu it and in you that is of to the\n",
      "    comp.graphics: edu in graphics it is for and of to the\n",
      "    sci.space: edu it that is in and space to of the\n",
      "    talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "  You can now see many things that these features have overfit to:\n",
      "\n",
      "  - Almost every group is distinguished by whether headers such as\n",
      "    ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "  - Another significant feature involves whether the sender is affiliated with\n",
      "    a university, as indicated either by their headers or their signature.\n",
      "  - The word \"article\" is a significant feature, based on how often people quote\n",
      "    previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "    wrote:\"\n",
      "  - Other features match the names and e-mail addresses of particular people who\n",
      "    were posting at the time.\n",
      "\n",
      "  With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "  barely have to identify topics from text at all, and they all perform at the\n",
      "  same high level.\n",
      "\n",
      "  For this reason, the functions that load 20 Newsgroups data provide a\n",
      "  parameter called **remove**, telling it what kinds of information to strip out\n",
      "  of each file. **remove** should be a tuple containing any subset of\n",
      "  ``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "  blocks, and quotation blocks respectively.\n",
      "\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "    0.77310...\n",
      "\n",
      "  This classifier lost over a lot of its F-score, just because we removed\n",
      "  metadata that has little to do with topic classification.\n",
      "  It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.76995...\n",
      "\n",
      "  Some other classifiers cope better with this harder version of the task. Try the\n",
      "  :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "  example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. rubric:: Data Considerations\n",
      "\n",
      "The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "discussion sparked by the death of George Floyd and a national reckoning over\n",
      "race and colonialism, the Cleveland Indians have decided to change their\n",
      "name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "American-themed name.\"\n",
      "\n",
      "https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. rubric:: Recommendation\n",
      "\n",
      "- When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "  by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "- This text dataset contains data which may be inappropriate for certain NLP\n",
      "  applications. An example is listed in the \"Data Considerations\" section\n",
      "  above. The challenge with using current text datasets in NLP for tasks such\n",
      "  as sentence completion, clustering, and other applications is that text\n",
      "  that is culturally biased and inflammatory will propagate biases. This\n",
      "  should be taken into consideration when using the dataset, reviewing the\n",
      "  output, and the bias should be documented.\n",
      "\n",
      ".. rubric:: Examples\n",
      "\n",
      "* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n",
      "--------------------------------------------------\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10\n",
      "18846\n",
      "--------------------------------------------------\n",
      "[10  3 17 ...  3  1  7]\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "# 下面是比较大的数据，需要下载一会，20类新闻\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "# print(news.feature_names)  #这个没有\n",
    "print(news.DESCR)\n",
    "print('-' * 50)\n",
    "print(news.data[0])\n",
    "print(news.target[0])\n",
    "print(len(news.data))\n",
    "print('-' * 50)\n",
    "print(news.target)\n",
    "print(min(news.target), max(news.target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boston 房价数据集\n",
    "\n",
    "这个数据集因为一些政治不正确的原因被官方删除了，因此需要一些替代手段。\n",
    "\n",
    "> The Boston housing prices dataset has an ethical problem: asinvestigated in [1], the authors of this dataset engineered anon-invertible variable \"B\" assuming that racial self-segregation had apositive impact on house prices [2]. Furthermore the goal of theresearch that led to the creation of this dataset was to study theimpact of air quality but it did not give adequate demonstration of thevalidity of this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "(506, 13)\n",
      "--------------------------------------------------\n",
      "目标值\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# 接着来看回归的数据,是波士顿房价\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# lb = load_boston()\n",
    "\n",
    "print(\"获取特征值\")\n",
    "print(data[0])\n",
    "print(data.shape)\n",
    "print('-' * 50)\n",
    "print(\"目标值\")\n",
    "print(target)\n",
    "# print(lb.DESCR)\n",
    "# print(lb.feature_names)\n",
    "print('-' * 50)\n",
    "# 回归问题没这个,打印这个会报错\n",
    "# print(lb.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 分类估计器\n",
    "\n",
    "在 sklearn 中，估计器(estimator)是一个重要的角色，分类器和回归器都属于 estimator，是一类实现了算法的 API.\n",
    "\n",
    "用于分类的估计器：\n",
    "- ``sklearn.neighbors`` k-近邻算法\n",
    "- ``sklearn.naive_bayes`` 朴素贝叶斯\n",
    "- ``sklearn.linear_model.LogisticRegression`` 逻辑回归\n",
    "- ``sklearn.tree`` 决策树与随机森林\n",
    "\n",
    "用于回归的估计器：\n",
    "- ``sklearn.linear_model.LinearRegression`` 线性回归\n",
    "- ``sklearn.linear_model.Ridge`` 岭回归\n",
    "\n",
    "估计器的工作流程：\n",
    "\n",
    "![pic1](./1.png)\n",
    "\n",
    "# knn\n",
    "\n",
    "下面简单介绍 k 近邻算法，此算法在 [人工智能导论](https://dropsong.github.io/posts/6f3f8819.html) 中亦有记载。\n",
    "\n",
    "算法的思想是：如果一个样本在特征空间中与其最相似的 k 个（即特征空间中最邻近）样本大多数属于某一个类别，则该样本也属于这个类别。\n",
    "\n",
    "**接下来看一个 k 近邻算法实例 - 预测入住位置** ：\n",
    "https://www.kaggle.com/c/facebook-v-predicting-check-ins\n",
    "\n",
    "下面的代码只是一个非常简单的版本，并且极大地缩小了数据集范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       x       y  accuracy    time    place_id\n",
      "0       0  0.7941  9.0809        54  470702  8523065625\n",
      "1       1  5.9567  4.7968        13  186555  1757726713\n",
      "2       2  8.3078  7.0407        74  322648  1137537235\n",
      "3       3  7.3665  2.5165        65  704587  6567393236\n",
      "4       4  4.0961  1.1307        31  472130  7440663949\n",
      "5       5  3.8099  1.9586        75  178065  6289802927\n",
      "6       6  6.3336  4.3720        13  666829  9931249544\n",
      "7       7  5.7409  6.7697        85  369002  5662813655\n",
      "8       8  4.3114  6.9410         3  166384  8471780938\n",
      "9       9  6.3414  0.0758        65  400060  1253803156\n",
      "(29118021, 6)\n",
      "600        1970-01-01 18:09:40\n",
      "957        1970-01-10 02:11:10\n",
      "4345       1970-01-05 15:08:02\n",
      "4735       1970-01-06 23:03:03\n",
      "5580       1970-01-09 11:26:50\n",
      "                   ...        \n",
      "29100203   1970-01-01 10:33:56\n",
      "29108443   1970-01-07 23:22:04\n",
      "29109993   1970-01-08 15:03:14\n",
      "29111539   1970-01-04 00:53:41\n",
      "29112154   1970-01-08 23:01:07\n",
      "Name: time, Length: 17710, dtype: datetime64[ns]\n",
      "--------------------------------------------------\n",
      "DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',\n",
      "               '1970-01-05 15:08:02', '1970-01-06 23:03:03',\n",
      "               '1970-01-09 11:26:50', '1970-01-02 16:25:07',\n",
      "               '1970-01-04 15:52:57', '1970-01-01 10:13:36',\n",
      "               '1970-01-09 15:26:06', '1970-01-08 23:52:02',\n",
      "               ...\n",
      "               '1970-01-07 10:03:36', '1970-01-09 11:44:34',\n",
      "               '1970-01-04 08:07:44', '1970-01-04 15:47:47',\n",
      "               '1970-01-08 01:24:11', '1970-01-01 10:33:56',\n",
      "               '1970-01-07 23:22:04', '1970-01-08 15:03:14',\n",
      "               '1970-01-04 00:53:41', '1970-01-08 23:01:07'],\n",
      "              dtype='datetime64[ns]', name='time', length=17710, freq=None)\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "--------------------------------------------------\n",
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[17710 rows x 8 columns]\n",
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[16918 rows x 8 columns]\n",
      "x.shapes = \n",
      "(16918, 6)\n",
      "x.columns = \n",
      "Index(['x', 'y', 'accuracy', 'day', 'hour', 'weekday'], dtype='object')\n",
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n",
      "--------------------------------------------------\n",
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n",
      "预测的目标签到位置为： [5689129232 1097200869 2355236719 ... 4932578245 6424972551 4492549925]\n",
      "预测的准确率: 0.4806146572104019\n"
     ]
    }
   ],
   "source": [
    "# K近邻\n",
    "\"\"\"\n",
    "K-近邻预测用户签到位置\n",
    ":return:None\n",
    "\"\"\"\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.shape)\n",
    "# 处理数据\n",
    "# 1、缩小数据,查询数据,为了减少计算时间\n",
    "data = data.query(\"x > 1.0 &  x < 1.25 & y > 2.5 & y < 2.75\")\n",
    "\n",
    "# 处理时间的数据\n",
    "time_value = pd.to_datetime(data['time'], unit='s')\n",
    "\n",
    "print(time_value)  # 最大时间是1月10号\n",
    "#\n",
    "# 把日期格式转换成 字典格式，把年，月，日，时，分，秒转换为字典格式，\n",
    "time_value = pd.DatetimeIndex(time_value)\n",
    "#\n",
    "print('-' * 50)\n",
    "print(time_value)\n",
    "print('-' * 50)\n",
    "# 构造一些特征，执行的警告是因为我们的操作是复制，loc是直接放入\n",
    "print(type(data))\n",
    "# data['day'] = time_value.day\n",
    "# data['hour'] = time_value.hour\n",
    "# data['weekday'] = time_value.weekday\n",
    "#日期，是否是周末，小时对于个人行为的影响是较大的\n",
    "data.insert(data.shape[1], 'day', time_value.day)\n",
    "data.insert(data.shape[1], 'hour', time_value.hour)\n",
    "data.insert(data.shape[1], 'weekday', time_value.weekday)\n",
    "\n",
    "#\n",
    "# 把时间戳特征删除\n",
    "data = data.drop(['time'], axis=1)\n",
    "print('-' * 50)\n",
    "print(data)\n",
    "#\n",
    "# 把签到数量少于n个目标位置删除\n",
    "place_count = data.groupby('place_id').count()\n",
    "# 把index变为0,1,2，3,4,5,6这种效果，从零开始拍，原来的index是地点\n",
    "# 只选择去的人大于3的数据，认为1,2,3的是噪音\n",
    "tf = place_count[place_count.row_id > 3].reset_index()\n",
    "# 根据设定的地点目标值，对原本的样本进行过滤\n",
    "data = data[data['place_id'].isin(tf.place_id)]\n",
    "print(data)\n",
    "# # 取出数据当中的特征值和目标值\n",
    "y = data['place_id']\n",
    "# 删除目标值，保留特征值，\n",
    "x = data.drop(['place_id'], axis=1)\n",
    "# 删除无用的特征值\n",
    "x = x.drop(['row_id'], axis=1)\n",
    "print('x.shapes = ')\n",
    "print(x.shape)\n",
    "print('x.columns = ')\n",
    "print(x.columns)\n",
    "# 进行数据的分割训练集合测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# 特征工程（标准化）,下面3行注释，一开始我们不进行标准化，看下效果，目标值要不要标准化？\n",
    "std = StandardScaler()\n",
    "# #\n",
    "# # # 对测试集和训练集的特征值进行标准化,服务于knn fit\n",
    "x_train = std.fit_transform(x_train)\n",
    "# # transform返回的是copy，不在原有的输入对象中去修改\n",
    "# print(id(x_test))\n",
    "print(std.mean_)\n",
    "print(std.var_)\n",
    "x_test = std.transform(x_test)  #transfrom不再进行均值和方差的计算，是在原有的基础上去标准化\n",
    "print('-' * 50)\n",
    "# print(id(x_test))\n",
    "print(std.mean_)\n",
    "print(std.var_)\n",
    "# # 进行算法流程 # 超参数，可以通过设置n_neighbors=5，来调整结果好坏\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# # fit， predict,score，训练\n",
    "knn.fit(x_train, y_train)\n",
    "# # #\n",
    "# # # 得出预测结果\n",
    "y_predict = knn.predict(x_test)\n",
    "# #\n",
    "print(\"预测的目标签到位置为：\", y_predict)\n",
    "# # #\n",
    "# # # # 得出准确率\n",
    "print(\"预测的准确率:\", knn.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k 值取多大？有什么影响？\n",
    "- k 值取很小：容易受异常点影响\n",
    "- k 值取很大：容易受最近数据太多导致比例变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-01-10 02:23:38\n"
     ]
    }
   ],
   "source": [
    "print(max(time_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-近邻算法优点：\n",
    "- 简单有效\n",
    "- 重新训练的代价低\n",
    "- 适合类域交叉样本\n",
    "  - KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN 方法较其他方法更为适合。\n",
    "- 适合大样本自动分类\n",
    "  - 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n",
    "\n",
    "k-近邻算法缺点：\n",
    "- 惰性学习\n",
    "  - KNN 算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多\n",
    "- 类别评分不是规格化（没有严格的数学推导）\n",
    "- 输出可解释性不强（例如决策树的输出可解释性就较强）\n",
    "- 对不均衡的样本不擅长\n",
    "  - 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。\n",
    "- 计算量较大\n",
    "  - 目前常用的解决方法是事先对已知样本点进行裁剪，事先去除对分类作用不大的样本。\n",
    "\n",
    "k-近邻算法实现：加快搜索速度——基于算法的改进 **KDTree**, API 接口里面有实现。\n",
    "\n",
    "# 网格搜索\n",
    "\n",
    "此内容在 [人工智能导论](https://dropsong.github.io/posts/6f3f8819.html) 中亦有记载。搜索“k-折交叉验证（K-CV）”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyue/Documents/myvenv/mlvenv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上准确率： 0.47966903073286055\n",
      "在交叉验证当中最好的结果： 0.4596467484726366\n",
      "选择最好的模型是： KNeighborsClassifier(n_neighbors=10)\n",
      "每个超参数每次交叉验证的结果： {'mean_fit_time': array([0.00663495, 0.00570822, 0.00563606, 0.00566483, 0.00570917]), 'std_fit_time': array([1.30730368e-03, 1.65227114e-05, 6.12806352e-05, 4.42359101e-05,\n",
      "       5.66617525e-05]), 'mean_score_time': array([0.08330512, 0.08822219, 0.10076253, 0.10499819, 0.11136285]), 'std_score_time': array([0.00067103, 0.0002986 , 0.00023517, 0.00053634, 0.00021282]), 'param_n_neighbors': masked_array(data=[3, 5, 10, 12, 15],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value=999999), 'params': [{'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 10}, {'n_neighbors': 12}, {'n_neighbors': 15}], 'split0_test_score': array([0.44468085, 0.4607565 , 0.46170213, 0.45650118, 0.45508274]), 'split1_test_score': array([0.43390873, 0.45660913, 0.45542681, 0.45329865, 0.44809648]), 'split2_test_score': array([0.43982029, 0.45684559, 0.4618113 , 0.45897375, 0.46062899]), 'mean_test_score': array([0.43946996, 0.45807041, 0.45964675, 0.45625786, 0.45460274]), 'std_test_score': array([0.00440467, 0.00190181, 0.00298428, 0.00232323, 0.00512762]), 'rank_test_score': array([5, 2, 1, 3, 4], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# 构造一些参数的值进行搜索\n",
    "param = {\"n_neighbors\": [3, 5, 10, 12, 15]}\n",
    "\n",
    "# 进行网格搜索，cv=3是3折交叉验证，用其中2折训练，1折验证\n",
    "gc = GridSearchCV(knn, param_grid=param, cv=3)\n",
    "\n",
    "gc.fit(x_train, y_train)  # 你给它的x_train，它又分为训练集，验证集\n",
    "\n",
    "# 预测准确率，为了给大家看看\n",
    "print(\"在测试集上准确率：\", gc.score(x_test, y_test))\n",
    "\n",
    "print(\"在交叉验证当中最好的结果：\", gc.best_score_)\n",
    "\n",
    "print(\"选择最好的模型是：\", gc.best_estimator_)\n",
    "\n",
    "print(\"每个超参数每次交叉验证的结果：\", gc.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类模型的评估\n",
    "\n",
    "## 基础知识\n",
    "\n",
    "前置知识 [参考此处](https://dropsong.github.io/posts/6f3f8819.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0) 。\n",
    "\n",
    "**精确率(Precision)**：预测结果为正例样本中真实为正例的比例（**查得准**），TP/(TP+FP)\n",
    "\n",
    "**召回率**：真实为正例的样本中预测结果为正例的比例（**查的全，对正样本的区分能力**），TP/(TP+FN)\n",
    "- 医院非常重视召回率\n",
    "\n",
    "**F1-score**，综合考虑了精确率和召回率，反映了模型的**稳健型**\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2 TP}{2 TP + FN + FP} = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "## ROC 曲线与 AUC 值概述\n",
    "\n",
    "**AUC（Area Under roc Curve）** 是一种用来度量分类模型好坏的一个标准。这样的标准其实有很多，例如：大约 10 年前在 machine learning 文献中一统天下的标准：分类精度；在信息检索(IR)领域中常用的 recall 和 precision，等等。其实，度量反应了人们对”好”的分类结果的追求，同一时期的不同的度量反映了人们对什么是”好”这个最根本问题的不同认识，而不同时期流行的度量则反映了人们认识事物的深度的变化。\n",
    "\n",
    "近年来，随着 machine learning 的相关技术从实验室走向实际应用，一些实际的问题对度量标准提出了新的需求。特别的，现实中样本在不同类别上的不均衡分布(class distribution imbalance problem)。使得 accuracy 这样的传统的度量标准不能恰当的反应分类器的 performance。举个例子：测试样本中有A类样本 90 个，B 类样本 10 个。分类器 C1 把所有的测试样本都分成了 A 类，分类器 C2 把 A 类的 90 个样本分对了 70 个，B 类的 10 个样本分对了 5 个。则 C1 的分类精度为 90%，C2 的分类精度为 75% 。但是，显然 C2 更有用些。另外，在一些分类问题中犯不同的错误代价是不同的(cost sensitive learning)。这样，默认 0.5 为分类阈值的传统做法也显得不恰当了。\n",
    "\n",
    "为了解决上述问题，人们从医疗分析领域引入了一种新的分类模型 performance 评判方法，ROC 分析。ROC 分析本身就是一个很丰富的内容，有兴趣的读者可以自行 Google，这里只做些简单的概念性的介绍。\n",
    "\n",
    "ROC 的全名叫做 Receiver Operating Characteristic，其主要分析工具是一个画在二维平面上的曲线，ROC curve. 平面的横坐标是 false positive rate(FPR)，纵坐标是 true positive rate(TPR)。对某个分类器而言，我们可以根据其在测试样本上的表现得到一个 TPR 和 FPR 点对。这样，此分类器就可以映射成 ROC 平面上的一个点。调整这个分类器分类时候使用的阈值，我们就可以得到一个经过(0, 0)，(1, 1)的曲线，这就是此分类器的 ROC 曲线。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。因为(0, 0)和(1, 1)连线形成的 ROC 曲线实际上代表的是一个随机分类器。如果很不幸，你得到一个位于此直线下方的分类器的话，一个直观的补救办法就是把所有的预测结果反向，即：分类器输出结果为正类，则最终分类的结果为负类，反之，则为正类。虽然，用 ROC curve 来表示分类器的 performance 很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。于是 Area Under roc Curve(AUC) 就出现了。顾名思义，AUC 的值就是处于 ROC curve 下方的那部分面积的大小。通常，AUC 的值介于 0.5 到 1.0 之间，**较大的 AUC 代表了较好的 performance.**\n",
    "\n",
    "## ROC 的动机\n",
    "\n",
    "对于 01 两类分类问题，一些分类器得到的结果往往不是 01 这样的标签，如神经网络得到诸如 0.5，0.8 这样的分类结果。这时，我们人为取一个阈值，比如 0.4，那么小于 0.4 的归为 0 类，大于等于 0.4 的归为 1 类，可以得到一个分类结果。同样，这个阈值我们可以取 0.1 或 0.2 等等。取不同的阈值，最后得到的分类情况也就不同。如下面这幅图：\n",
    "\n",
    "![pic2](./2.png)\n",
    "\n",
    "蓝色表示原始为负类分类得到的统计图，红色表示原始为正类得到的统计图。那么我们取一条直线，直线左边分为负类，直线右边分为正类，这条直线也就是我们所取的阈值。阈值不同，可以得到不同的结果，但是由分类器决定的统计图始终是不变的。这时候就需要一个独立于阈值，只与分类器有关的评价指标，来衡量特定分类器的好坏。还有在类不平衡的情况下，如正样本有 90 个，负样本有 10 个，直接把所有样本分类为正样本，得到识别率为 90%，但这显然是没有意义的。如上就是 ROC 曲线的动机。\n",
    "\n",
    "## ROC 的定义\n",
    "\n",
    "ROC 曲线最初是运用在军事上，当前在医学领域使用非常广泛，因此指标称“阴性”，“阳性”。\n",
    "\n",
    "| 名词     | 说明                                                                 |\n",
    "|----------|----------------------------------------------------------------------|\n",
    "| 特异性   | 阴性人群中，检测出阴性的几率。也称真阴性率。                        |\n",
    "| 误报率   | 阴性人群中，检测为阳性的几率。也称假阳性率。（1-特异性）         |\n",
    "| 敏感度   | 阳性人群中，检测出阳性的几率。也称真阳性率。                        |\n",
    "| 假阴性率 | 阳性人群中，检测为阴性的几率。也称假阴性率。                        |\n",
    "\n",
    "ROC 曲线的绘制是对所有测试样本的“误报率”以及“敏感度”绘制曲线。取**不同阈值时**对每个样本计算的 “误报率”**（假阳）作为 X 轴坐标**，“敏感度”**（真阳）作为 Y 轴坐标**。\n",
    "\n",
    "因为样本的“误报率”（假阳）越小越好，“敏感度”（真阳）越大越好，即 X 坐标越小越好，Y 坐标越大越好，因此绘制的 ROC 曲线越靠近象限左上角越好。\n",
    "\n",
    "## ROC 的图形化表示\n",
    "\n",
    "![pic3](./3.png)\n",
    "\n",
    "我们可以看出：左上角的点（TPR=1，FPR=0），为完美分类，也就是这个医生医术高明，诊断全对；点A（TPR>FPR），医生A的判断大体是正确的。中线上的点B（TPR=FPR），也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C（TPR<FPR），这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。\n",
    "\n",
    "**上图中一个阈值，得到一个点。**现在我们**需要一个独立于阈值的评价指标**来衡量这个医生的医术如何，也就是遍历所有的阈值，得到ROC曲线。还是一开始的那幅图，假设如下就是某个医生的诊断统计图，直线代表阈值。我们遍历所有的阈值，能够在ROC平面上得到如下的ROC曲线。\n",
    "\n",
    "![pic4](./4.png)\n",
    "\n",
    "曲线距离左上角越近，证明分类器效果越好。\n",
    "\n",
    "![pic5](./5.png)\n",
    "\n",
    "如上，是三条ROC曲线，在0.23处取一条直线。那么，在同样的FPR=0.23的情况下，红色分类器得到更高的TPR。也就表明，ROC越往上，分类器效果越好。我们用一个标量值AUC来量化它。\n",
    "\n",
    "## AUC 值\n",
    "\n",
    "AUC 值为 ROC 曲线所覆盖的区域面积，显然，AUC 越大，分类器分类效果越好。\n",
    "- ``AUC = 1``，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n",
    "- ``0.5 < AUC < 1``，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\n",
    "- ``AUC = 0.5``，跟随机猜测一样（例：丢铜板），模型没有预测价值。\n",
    "- ``AUC < 0.5``，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。\n",
    "\n",
    "**AUC 值的物理意义：**\n",
    "假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。\n",
    "\n",
    "**AUC 值的计算：**\n",
    "1. 第一种方法：AUC为ROC曲线下的面积，那我们直接计算面积可得。面积为一个个小的梯形面积之和，计算的精度与阈值的精度有关。\n",
    "2. 第二种方法：根据AUC的物理意义，我们计算正样本score大于负样本的score的概率。取 ``N*M``（N为正样本数，M为负样本数）个二元组，比较score，最后得到AUC。时间复杂度为 $O(NM)$ 。\n",
    "3. 第三种方法：与第二种方法相似，直接计算正样本score大于负样本的score的概率。我们首先把所有样本按照score排序，依次用rank表示他们，如最大score的样本，rank=n(n=N+M)，其次为n-1。那么对于正样本中rank最大的样本（rank_max），有M-1个其他正样本比他score小，那么就有(rank_max-1)-(M-1)个负样本比他score小。其次为(rank_second-1)-(M-2)。最后我们得到正样本大于负样本的概率为：\n",
    "    $$\n",
    "    \\frac{\\sum_{\\text{所有正样本}} \\mathrm{rank} - M(M+1) / 2}{M \\cdot N}\n",
    "    $$\n",
    "时间复杂度为 O(N+M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯进行文本分类\n",
    "\n",
    "注意，[主观贝叶斯](https://dropsong.github.io/posts/6f3f8819.html#%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86)和**朴素贝叶斯**是不同的概念。\n",
    "\n",
    "考虑这样的需求，我们需要对文章分类。根据一篇文章中出现的词，大致推断这是什么文章（科技、娱乐、...）。\n",
    "\n",
    "$$\n",
    "P(C | W) = \\frac{P( W | C)P(C)}{P(W)}\n",
    "$$\n",
    "\n",
    "其中，W 为给定文档的特征值（频数统计,预测文档提供），$C$ 为文档类别。\n",
    "\n",
    "假设训练集统计结果（指定统计词频）如下：\n",
    "\n",
    "| 特征\\统计 | 科技 | 娱乐 | 汇总（求和） |\n",
    "| --- | --- | --- | --- |\n",
    "| “商场” | 9 | 51 | 60 |\n",
    "| “影院” | 8 | 56 | 64 |\n",
    "| “支付宝” | 20 | 15 | 35 |\n",
    "| “云计算” | 63 | 0 | 63 |\n",
    "| 汇总(求和)  | 100 | 121 | 221 |\n",
    "\n",
    "现有一篇被预测文档：出现了影院，支付宝，云计算，计算属于科技、娱乐的类别概率？\n",
    "\n",
    "科技：\n",
    "\n",
    "$$\n",
    "P(\\text{影院，支付宝，云计算} | \\text{科技}) \\cdot P(\\text{科技}) = \\frac{8}{100} \\cdot \\frac{20}{100} \\cdot \\frac{63}{100} \\cdot (\\frac{100}{221})\n",
    "$$\n",
    "\n",
    "娱乐：\n",
    "\n",
    "$$\n",
    "P(\\text{影院，支付宝，云计算} | \\text{娱乐}) \\cdot P(\\text{娱乐}) = \\frac{56}{121} \\cdot \\frac{15}{121} \\cdot \\frac{0}{121} \\cdot (\\frac{121}{221}) = 0\n",
    "$$\n",
    "\n",
    "问题：**若属于某个类别为 0，合适吗？**\n",
    "\n",
    "上面的例子我们得到娱乐概率为 0，这是**不合理**的，如果词频列表里面有很多出现次数都为 0，很可能计算结果都为零。\n",
    "\n",
    "解决方法：**拉普拉斯平滑系数** 。\n",
    "\n",
    "$$\n",
    "P(F_1 | C) = \\frac{N_i + \\alpha}{N + \\alpha m}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 为指定的系数，一般为 1. m 为训练文档中统计出的特征词个数。\n",
    "\n",
    "这样，娱乐类的计算方式就变为：\n",
    "\n",
    "$$\n",
    "P(\\text{影院，支付宝，云计算} | \\text{娱乐}) \\cdot P(\\text{娱乐}) = \\frac{56 + 1}{121 + 1 \\cdot 4} \\cdot \\frac{15 + 1}{121 + 1 \\cdot 4} \\cdot \\frac{0 + 1}{121 + 1 \\cdot 4} \\cdot (\\frac{121}{221}) = 0.001\n",
    "$$\n",
    "\n",
    "注意，**科技类的计算方式也需要进行拉普拉斯**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "[10  3 17 ...  3  1  7]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "朴素贝叶斯进行文本分类\n",
    ":return: None\n",
    "\"\"\"\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "\n",
    "print(len(news.data))\n",
    "print(news.target)\n",
    "print(news.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 进行数据分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 对数据集进行特征抽取\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "x_train = tf.fit_transform(x_train)\n",
    "\n",
    "# print(tf.get_feature_names())\n",
    "\n",
    "x_test = tf.transform(x_test)\n",
    "\n",
    "# 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "\n",
    "print(x_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的文章类别为： [16 19 18 ... 13  7 14]\n",
      "准确率为： 0.8518675721561969\n",
      "每个类别的精确率和召回率：                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.91      0.77      0.83       199\n",
      "           comp.graphics       0.83      0.79      0.81       242\n",
      " comp.os.ms-windows.misc       0.89      0.83      0.86       263\n",
      "comp.sys.ibm.pc.hardware       0.80      0.83      0.81       262\n",
      "   comp.sys.mac.hardware       0.90      0.88      0.89       234\n",
      "          comp.windows.x       0.92      0.85      0.88       230\n",
      "            misc.forsale       0.96      0.67      0.79       257\n",
      "               rec.autos       0.90      0.87      0.88       265\n",
      "         rec.motorcycles       0.90      0.95      0.92       251\n",
      "      rec.sport.baseball       0.89      0.96      0.93       226\n",
      "        rec.sport.hockey       0.95      0.98      0.96       262\n",
      "               sci.crypt       0.76      0.97      0.85       257\n",
      "         sci.electronics       0.84      0.80      0.82       229\n",
      "                 sci.med       0.97      0.86      0.91       249\n",
      "               sci.space       0.92      0.96      0.94       256\n",
      "  soc.religion.christian       0.55      0.98      0.70       243\n",
      "      talk.politics.guns       0.76      0.96      0.85       234\n",
      "   talk.politics.mideast       0.93      0.99      0.96       224\n",
      "      talk.politics.misc       0.98      0.56      0.72       197\n",
      "      talk.religion.misc       0.97      0.26      0.41       132\n",
      "\n",
      "                accuracy                           0.85      4712\n",
      "               macro avg       0.88      0.84      0.84      4712\n",
      "            weighted avg       0.87      0.85      0.85      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "mlt.fit(x_train, y_train)\n",
    "\n",
    "y_predict = mlt.predict(x_test)\n",
    "\n",
    "print(\"预测的文章类别为：\", y_predict)\n",
    "\n",
    "# 得出准确率,这个是很难提高准确率，为什么呢？\n",
    "print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "# 目前这个场景我们不需要召回率，support是划分为那个类别的有多少个样本\n",
    "print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict, target_names=news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC指标： 0.8827602448315142\n"
     ]
    }
   ],
   "source": [
    "# 把0-19总计20个分类，变为0和1\n",
    "y_test = np.where(y_test == 0, 1, 0) # 0 改成 1,其他的改成 0，这样就转化为二分类了\n",
    "y_predict = np.where(y_predict == 0, 1, 0)\n",
    "# roc_auc_score的y_test只能是二分类,针对多分类如何计算AUC\n",
    "print(\"AUC指标：\", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'ñaustin' 'ýé' 'ÿhooked']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "153196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del x_train\n",
    "# del x_test\n",
    "# del y_test\n",
    "# del y_predict\n",
    "print(tf.get_feature_names_out())\n",
    "len(tf.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
